# -*- coding: utf-8 -*-
"""CS231A_Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yZ48ukggqn8W9bNvtAbhZHuc5BhM9SiS

# Install and Import Needed Packages
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive

drive.mount('/content/drive', force_remount=True)

# Enter the foldername in your Drive where you have saved the unzipped
# '.py' files from the p2 folder
# e.g. 'cs231a/pset3/p2'
FOLDERNAME = 'cs231A_final_project'

assert FOLDERNAME is not None, "[!] Enter the foldername."

# %cd drive/MyDrive
# %cd $FOLDERNAME

# Install required packages. 
!pip install -qq -U diffusers==0.11.1 transformers ftfy gradio accelerate
!pip install --upgrade paddlepaddle
!pip install --upgrade paddlehub
!pip install pypotree
!pip install open3d
!pip install Pillow==9.0.0

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
import gradio as gr
from diffusers import StableDiffusionInpaintPipeline
import torch
from transformers import DPTFeatureExtractor, DPTForDepthEstimation, VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
from PIL import Image
import requests
from io import BytesIO
import numpy as np
import cv2

# https://huggingface.co/spaces/PaddlePaddle/U2Net
import torch as th
import paddlehub as hub

"""# Load Models"""

matting_model = hub.Module(name='U2Net')
# test_matte = fg_alpha_matte(np.array(test_image))[0]['mask']
# display_image(test_matte)

device = "cuda"
model_path = "runwayml/stable-diffusion-inpainting"

outpaint_model = StableDiffusionInpaintPipeline.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
).to(device)

text_model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
text_feature_extractor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
text_model.to(device)

# We use Vision Transformers for Depth Prediction
# TODO: How was it trained?
# https://huggingface.co/Intel/dpt-hybrid-midas

depth_feature_extractor = DPTFeatureExtractor.from_pretrained("Intel/dpt-hybrid-midas", low_cpu_mem_usage=True)
depth_model = DPTForDepthEstimation.from_pretrained("Intel/dpt-hybrid-midas")

from google.colab import output
output.enable_custom_widget_manager()

"""#Helper Functions"""

def display_image(image):
  imgplot = plt.imshow(image, cmap="gray")
  plt.show()

def download_image(url):
  response = requests.get(url)
  return Image.open(BytesIO(response.content)).convert("RGB")

def pad_image(image, expected_shape=(512, 512)):
  # Generate Mask
  pad_length = 100
  width, height = image.size
  new_width = width + pad_length + pad_length
  new_height = height + pad_length + pad_length

  border_padding = Image.new(image.mode, (new_width, new_height), (255, 255, 255))
  inner_image_mask = Image.new(image.mode, (width, height), (0, 0, 0))
  
  padded_image = border_padding.copy()
  padded_image.paste(image, (pad_length, pad_length))
  padded_image = padded_image.resize(expected_shape)

  image_mask = border_padding.copy()
  image_mask.paste(inner_image_mask, (pad_length, pad_length))
  image_mask = image_mask.resize(expected_shape)

  return padded_image, image_mask

def image_grid(imgs, rows, cols):
    assert len(imgs) == rows*cols

    w, h = imgs[0].size
    grid = Image.new('RGB', size=(cols*w, rows*h))
    grid_w, grid_h = grid.size
    
    for i, img in enumerate(imgs):
        grid.paste(img, box=(i%cols*w, i//cols*h))
    return grid

"""# Load Input Image"""

img_url = "https://cdn.pixabay.com/photo/2016/03/23/15/00/ice-cream-1274894_960_720.jpg"
# img_url = "https://ichef.bbci.co.uk/news/640/cpsprodpb/160B4/production/_103229209_horsea.png"
# img_url = "https://images.unsplash.com/photo-1606107869722-d5cbadabe2f0?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1470&q=80"
input_image = download_image(img_url)
# input_image = test_image
# input_image = input_image.resize((512, 512))
padded_image, image_mask = pad_image(input_image)

expected_shape = np.asarray(input_image).shape

image_grid([input_image], 1, 1)

"""# Image to Text"""

def image_to_text(image):
  kwargs = {"max_length": 20, "num_beams": 4}

  pixel_values = text_feature_extractor(images=[image], return_tensors="pt").pixel_values
  pixel_values = pixel_values.to(device)
  output_ids = text_model.generate(pixel_values, **kwargs)

  labels = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
  labels = [label.strip() for label in labels]
  return labels[0]

"""# Apply Outpainting"""

def outpaint_image(image, image_mask):
  # In report note the effect of outpainting on edited images
  prompt = image_to_text(image) + " in natural background"
  print(f'{prompt = }')
  guidance_scale=7.5
  num_samples = 3
  generator = torch.Generator(device=device).manual_seed(20) # change the seed to get different results

  images = outpaint_model(
      prompt=prompt,
      image=image,
      mask_image=image_mask,
      guidance_scale=guidance_scale,
      generator=generator,
      num_images_per_prompt=num_samples,
  ).images

  return images

images = outpaint_image(padded_image, image_mask)
image_grid(images, 1, 3)

outpainted_image = images[0]
outpainted_image.save("outpainted_image.png","PNG")

figure_1 = [input_image, padded_image, image_mask, outpainted_image]
image_grid(figure_1, 1, len(figure_1))

outpainted_image = input_image

outpainted_image

"""# Monocular Depth Estimation"""

from PIL import ImageFilter

def get_diparity(outpainted_image, gaussian_blur=True):
  features = depth_feature_extractor(images=outpainted_image, return_tensors="pt")

  with torch.no_grad():
      output = depth_model(**features)
      predicted_depth = output.predicted_depth

  # interpolate to original size
  prediction = torch.nn.functional.interpolate(
      predicted_depth.unsqueeze(1),
      size=outpainted_image.size[::-1],
      mode="bicubic",
      align_corners=False,
  )

  # visualize the prediction
  disparity = prediction.squeeze().cpu().numpy()

  # Apply gaussian blurr
  if gaussian_blur:
    disparity = cv2.GaussianBlur(disparity, (5,5), cv2.BORDER_DEFAULT)
    pooling = th.nn.MaxPool1d(kernel_size=5, stride=1)
    disparity = pooling(th.from_numpy(disparity).float())
  disparity = cv2.resize(np.array(disparity), expected_shape[:2][::-1], interpolation = cv2.INTER_AREA)
  disparity = (disparity - disparity.min()) / (disparity.max() - disparity.min())
  np.savetxt('disparity.csv', disparity, delimiter=',') 
  return disparity

disparity = get_diparity(outpainted_image)
display_image(disparity)

"""

# Soft FG Pixel Visibility"""

from scipy import ndimage
import numpy as np
from tqdm import tqdm

def fg_pixel_visibility_map(depth):
  sobel_gradient_x = ndimage.sobel(depth, axis=0, mode='constant')
  sobel_gradient_y = ndimage.sobel(depth, axis=1, mode='constant')
  sobel_gradient = np.hypot(sobel_gradient_x, sobel_gradient_y)
  beta = 2
  pixel_visibility_map = np.exp(-beta * np.square(sobel_gradient))
  return pixel_visibility_map

display_image(fg_pixel_visibility_map(disparity))

"""#Improved Layering with Segmentation"""

def fg_alpha_matte(image):
  output = matting_model.Segmentation(
        images=[image],
        paths=None,
        batch_size=1,
        input_size=312,
        output_dir='output',
        visualization=True)[0]['mask']
  return (output - output.min()) / (output.max() - output.min())

output = fg_alpha_matte(np.array(outpainted_image))
foreground_segmentation = output
display_image(foreground_segmentation)

"""# View Outputs"""

print("Input Image")
display_image(input_image)
print("Outpainted Image")
display_image(outpainted_image)
print("Disparity Map")
display_image(disparity)
print("FG Visibility Map")
display_image(fg_pixel_visibility_map(disparity))
print("Matte Map")
display_image(foreground_segmentation)

"""# Foreground Layer"""

matte = fg_alpha_matte(np.array(outpainted_image))
foreground_segmentation = matte

M = foreground_segmentation
disparity = cv2.GaussianBlur(M, (5,5), cv2.BORDER_DEFAULT)
pooling = th.nn.MaxPool1d(kernel_size=5, stride=1)
M_prime = pooling(th.from_numpy(M).float())
M_prime = cv2.resize(np.array(M_prime), expected_shape[:2][::-1], interpolation = cv2.INTER_AREA)

depth_based_visibility_map = fg_pixel_visibility_map(disparity)

display_image((foreground_segmentation - M_prime))
display_image(depth_based_visibility_map)
display_image(M_prime)

display_image(depth_based_visibility_map + (foreground_segmentation - M_prime))

A = (foreground_segmentation - M_prime) * depth_based_visibility_map + depth_based_visibility_map
display_image(A)

stacked_A = np.stack((A, A, A)).transpose([1, 2, 0])
stacked_A = (stacked_A - stacked_A.min()) / (stacked_A.max() - stacked_A.min())

foreground_image = np.clip(outpainted_image, 0, 255).astype(np.uint8)
foreground_disparity = get_diparity(outpainted_image, False)
# display_image(foreground_image)

"""# Background Layer"""

np.array(M_prime).shape

def generate_background(outpainted_image, M_prime):
  # breakpoint()
  outpainted_image = outpainted_image.copy()
  outpainted_image = outpainted_image.resize((512, 512))
  M_prime = Image.fromarray(np.clip(M_prime * 255, 0, 255).astype(np.uint8)).resize((512, 512))

  prompt = "totally barren bare empty field no trees" # Nice
  guidance_scale = 10
  num_samples = 3
  generator = torch.Generator(device=device).manual_seed(0) # change the seed to get different results

  images = outpaint_model(
      prompt=prompt,
      image=outpainted_image,
      mask_image=M_prime,
      guidance_scale=guidance_scale,
      generator=generator,
      num_images_per_prompt=num_samples,
  ).images
  return images

assert np.array(M_prime).shape == np.array(outpainted_image).shape[:2]
print(np.array(M_prime).shape, np.array(outpainted_image).shape[:2])
images = generate_background(outpainted_image, M_prime)

image_grid(images, 1, 3)

background_rgb = images[2]
background_depth = get_diparity(background_rgb, False)
background_image = cv2.resize(np.array(background_rgb), expected_shape[:2][::-1], interpolation = cv2.INTER_AREA)

# Try combination
test_combo = stacked_A*foreground_image + (1 - stacked_A)*background_image
display_image(np.clip(test_combo, 0, 255).astype(np.uint8))

"""# Create Mesh"""

import open3d as o3d

def create_mesh(image, disparity):
  # generate non-square meshes
  image_length = min(image.shape[:2])
  print(image_length)
  points = np.zeros((image_length*image_length, 3))
  colors = np.zeros((image_length*image_length, 3))

  x = np.arange(0, image_length, 1)
  y = np.arange(0, image_length, 1)
  xv, yv = np.meshgrid(x, y)
  xv = np.expand_dims(xv, 2)
  yv = np.expand_dims(yv, 2)
  grid = np.concatenate((xv, yv), axis=2)
  grid = grid.reshape((image_length*image_length, 2))
  pixel_locations = grid.T.tolist()

  points[:, :2] = grid/image_length  
  points[:, 2] = disparity[pixel_locations]
  colors = image[pixel_locations]

  pcd = o3d.geometry.PointCloud()
  pcd.points = o3d.utility.Vector3dVector(points)
  pcd.colors = o3d.utility.Vector3dVector(colors/colors.max())
  pcd.estimate_normals()
  pcd.orient_normals_consistent_tangent_plane(15)
  mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd)
  return mesh

# new_shape = expected_shape[:2][::-1]
new_shape = (512, 512)

foreground_disparity =  cv2.resize(np.array(foreground_disparity),  new_shape)
foreground_image =  cv2.resize(np.array(foreground_image), new_shape)

foreground_mesh = create_mesh(foreground_image, foreground_disparity)
o3d.io.write_triangle_mesh("./foreground_mesh.ply", foreground_mesh)

background_depth =  cv2.resize(np.array(background_depth),  new_shape)
background_image =  cv2.resize(np.array(background_image), new_shape)

background_mesh = create_mesh(background_image, background_depth)
o3d.io.write_triangle_mesh("./background_mesh.ply", background_mesh)

foreground_visibility =  cv2.resize(np.clip(stacked_A*255, 0, 255).astype(np.uint8), new_shape)

foreground_visibility_mesh = create_mesh(foreground_visibility, foreground_disparity)
o3d.io.write_triangle_mesh("./foreground_visibility_mesh.ply", foreground_visibility_mesh)